{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of Natural Language Processing (NLP) with three different processing pipelines, each serving a distinct purpose. We'll create pipelines for text classification, named entity recognition (NER), and sentiment analysis using Python and popular NLP libraries like spaCy and TextBlob.\n",
        "\n",
        "This example demonstrates how you can create multiple NLP pipelines to perform different tasks on the same input text.\n",
        "\n",
        "- its possible to customize each pipeline to suit your specific NLP tasks and models.\n",
        "\n",
        "three different processing pipelines of example:\n",
        "\n",
        "**Text Classification Pipeline:**\n",
        "\n",
        " This pipeline takes a text as input and classifies it into a predefined category, such as \"Sports,\" \"Technology,\" or \"Health.\" You can replace the placeholder logic with your actual classification model.\n",
        "\n",
        "**Named Entity Recognition (NER) Pipeline:**\n",
        "\n",
        "This pipeline extracts named entities (e.g., people's names, locations, organizations) from the input text using spaCy's built-in named entity recognition capabilities.\n",
        "\n",
        "**Sentiment Analysis Pipeline:**\n",
        "\n",
        "This pipeline analyzes the sentiment of the input text and classifies it as \"Positive,\" \"Negative,\" or \"Neutral\" using the TextBlob library."
      ],
      "metadata": {
        "id": "wkT7TgGaasQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3DfFMjQmR39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the following commands to install spaCy and TextBlob\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TkEmNMmcmn9",
        "outputId": "95b62d16-743a-4a85-eeb5-b0a9a7a99dd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-11-22 15:54:33.068619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-22 15:54:33.068692: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-22 15:54:33.068735: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-22 15:54:33.083115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-22 15:54:34.343948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spacy** is a popular NLP (Natural Language Processing) library in Python that provides various NLP capabilities, including tokenization, part-of-speech tagging, named entity recognition, and more.\n",
        "\n",
        "**en_core_web_sm** is the name of the spaCy language model for English that loading in this code and this loaded model contains pre-trained word embeddings, linguistic rules, and statistical models that enable various NLP tasks like tokenization, part-of-speech tagging, dependency parsing, and named entity recognition.\n",
        "\n",
        "**SpaCy** takes the text input and processes it using a series of NLP tasks, which may include tokenization (splitting the text into individual words or tokens), part-of-speech tagging (identifying the grammatical category of each word), named entity recognition (identifying named entities like people, places, organizations), dependency parsing (determining the syntactic relationships between words)\n",
        "\n",
        "In spaCy, a named entity is a word or phrase that represents a specific type of object, such as a person's name, location, organization, date, etc.\n",
        "\n",
        "**TextBlob** is a Python library that provides a simple API for common natural language processing (NLP) tasks, including sentiment analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lv4Xw81ZeH3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries and define the three pipelines\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load spaCy models\n",
        "#en_core_web_sm is the name of the spaCy language model for English that loading in this code.\n",
        "#This model is a small and efficient model that includes word vectors and various linguistic annotations.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "# Define text classification pipeline\n",
        "def classify_text(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Add your text classification logic here based on the content of 'doc'\n",
        "    # Example: Determine if the text is related to sports, technology, or health.\n",
        "    # Initialize counters for each category\n",
        "    sports_count = 0\n",
        "    technology_count = 0\n",
        "    health_count = 0\n",
        "\n",
        "    # Define keywords associated with each category\n",
        "    sports_keywords = [\"sports\", \"football\", \"basketball\", \"athlete\", \"game\"]\n",
        "    technology_keywords = [\"technology\", \"innovation\", \"gadget\", \"software\", \"device\"]\n",
        "    health_keywords = [\"health\", \"wellness\", \"medicine\", \"fitness\", \"nutrition\"]\n",
        "\n",
        "    # Count the occurrence of keywords in the text\n",
        "    for token in doc:\n",
        "        if token.text.lower() in sports_keywords:\n",
        "            sports_count += 1\n",
        "        elif token.text.lower() in technology_keywords:\n",
        "            technology_count += 1\n",
        "        elif token.text.lower() in health_keywords:\n",
        "            health_count += 1\n",
        "\n",
        "    # Determine the category based on the highest keyword count\n",
        "    if sports_count > technology_count and sports_count > health_count:\n",
        "        return \"Sports\"\n",
        "    elif technology_count > sports_count and technology_count > health_count:\n",
        "        return \"Technology\"\n",
        "    elif health_count > sports_count and health_count > technology_count:\n",
        "        return \"Health\"\n",
        "    else:\n",
        "        return \"Unknown\"  # If no category is clearly dominant\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# doc: This variable is used to store the result of processing the input text with the spaCy model.\n",
        "# The result of processing \"nlp(text)\" is stored in the doc variable as a \"Doc\" object.\n",
        "\n",
        "# Define named entity recognition (NER) pipeline\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "\n",
        "# This part constructs a tuple (ent.text, ent.label_) for each named entity found in the document,\n",
        "# the first element of the tuple is the text of the named entity, and the second element is its label.\n",
        "# the entities variable will hold a list of named entities and their corresponding labels extracted from the processed document doc.\n",
        "\n",
        "# Define sentiment analysis pipeline\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    if sentiment_score > 0:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "#TextBlob object name is blob by passing the input text text to it."
      ],
      "metadata": {
        "id": "ZbSxIJvKczg2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Classification Pipeline (classify_text):**\n",
        "\n",
        "- This pipeline classifies text into categories (in this case, \"Sports,\" \"Technology,\" \"Health,\" or \"Unknown\") based on the presence of specific keywords related to each category.\n",
        "- It initializes counters for each category and defines keywords associated with each category.\n",
        "- It then counts the occurrence of keywords in the text and determines the category with the highest keyword count.\n",
        "\n",
        "**Named Entity Recognition (NER) Pipeline (extract_entities):**\n",
        "\n",
        "- This pipeline extracts named entities (e.g., people's names, locations, organizations) from the input text using spaCy's built-in named entity recognition capabilities.\n",
        "- It constructs tuples of named entities and their corresponding labels (e.g., \"PERSON,\" \"GPE\").\n",
        "\n",
        "**Sentiment Analysis Pipeline (analyze_sentiment):**\n",
        "\n",
        "- This pipeline analyzes the sentiment of the input text (whether it's positive, negative, or neutral) using TextBlob.\n",
        "- It calculates the sentiment polarity score using blob.sentiment.polarity and categorizes the text as \"Positive,\" \"Negative,\" or \"Neutral\" based on the polarity score."
      ],
      "metadata": {
        "id": "uOPsUA6mmPx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the Pipelines on Sample Text\n",
        "# Example text\n",
        "text = \"The new smartphone from XYZ Company is impressive, and it was launched at an event in New York yesterday.\"\n",
        "\n",
        "# Execute the pipelines\n",
        "classification_result = classify_text(text)\n",
        "ner_result = extract_entities(text)\n",
        "sentiment_result = analyze_sentiment(text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Text Classification Result:\", classification_result)\n",
        "print(\"Named Entity Recognition Result:\", ner_result)\n",
        "print(\"Sentiment Analysis Result:\", sentiment_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mniabXHKdHtH",
        "outputId": "021245c9-be97-4958-de16-da4f559f1330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Classification Result: Unknown\n",
            "Named Entity Recognition Result: [('XYZ Company', 'ORG'), ('New York', 'GPE'), ('yesterday', 'DATE')]\n",
            "Sentiment Analysis Result: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text 2\n",
        "text = \"The latest technology in sports equipment improves athlete performance.\"\n",
        "\n",
        "# Execute the pipelines\n",
        "classification_result = classify_text(text)\n",
        "ner_result = extract_entities(text)\n",
        "sentiment_result = analyze_sentiment(text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Text Classification Result:\", classification_result)\n",
        "print(\"Named Entity Recognition Result:\", ner_result)\n",
        "print(\"Sentiment Analysis Result:\", sentiment_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jAJGr_atBR",
        "outputId": "6893d628-c50a-41b6-8f19-aa5635a98df0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Classification Result: Sports\n",
            "Named Entity Recognition Result: []\n",
            "Sentiment Analysis Result: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  create the \"models\" directory first then saving files into it.\n",
        "\n",
        "- To save your model pickle files directly to Google Drive in a Google Colab environment, you need to mount your Google Drive and then specify the path to your Google Drive directory where you want to save the files.  "
      ],
      "metadata": {
        "id": "uZlISZoR1UWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model pickle files to the specified directory in your Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive (will prompt for authorization)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory path in your Google Drive\n",
        "# Change 'models' to your desired directory path in your Google Drive\n",
        "drive_models_directory = '/content/drive/My Drive/models'\n",
        "\n",
        "# Create the directory in Google Drive if it doesn't exist\n",
        "if not os.path.exists(drive_models_directory):\n",
        "    os.makedirs(drive_models_directory)\n",
        "\n",
        "# Save the text classification model to Google Drive\n",
        "with open(os.path.join(drive_models_directory, 'text_classification_model.pkl'), 'wb') as model_file:\n",
        "    pickle.dump(classify_text, model_file)\n",
        "\n",
        "# Save the NER model to Google Drive\n",
        "with open(os.path.join(drive_models_directory, 'ner_model.pkl'), 'wb') as model_file:\n",
        "    pickle.dump(extract_entities, model_file)\n",
        "\n",
        "# Save the sentiment analysis model to Google Drive\n",
        "with open(os.path.join(drive_models_directory, 'sentiment_analysis_model.pkl'), 'wb') as model_file:\n",
        "    pickle.dump(analyze_sentiment, model_file)\n",
        "# download model pickle files from the /content/models directory in Google Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhga6tp-xk19",
        "outputId": "07f7372f-9270-475d-e8d9-e627f579a19c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an API That Supports All Three Models\n",
        "\n",
        "In this modified API, we load all three models (classify_text_model, ner_model, and sentiment_analysis_model) and create separate routes to call each model. The API now returns results for all three tasks.\n",
        "\n",
        "**API Creation steps**:\n",
        "- creat new project in pycharm in new window (3pipeNLP)\n",
        "- connect to \" my - streamlit- environment \" from anaconda to 3pipeNLP\n",
        "- Create a model_loader.py file\n",
        "- copy API code in main part\n",
        "- file ---> new ---> directory ---> model ---> copy all 3 pickle file in model\n",
        "- new python file ---> streamlit-app.py"
      ],
      "metadata": {
        "id": "WoD5bMcoyHM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model_loader.py file\n",
        "\n",
        "In your project directory, create a Python file named model_loader.py. This file will contain functions for loading your models."
      ],
      "metadata": {
        "id": "bbDkFoxnC8wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def load_model(model_filename):\n",
        "    model_directory = 'model'\n",
        "    model_path = os.path.join(model_directory, model_filename)\n",
        "    with open(model_path, 'rb') as model_file:\n",
        "        return pickle.load(model_file)\n",
        "\n",
        "# Load the models\n",
        "text_classification_model = load_model('text_classification_model.pkl')\n",
        "ner_model = load_model('ner_model.pkl')\n",
        "sentiment_analysis_model = load_model('sentiment_analysis_model.pkl')\n"
      ],
      "metadata": {
        "id": "9ppucjPcC56y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify your main.py file\n",
        "\n",
        "In your main.py file, import the functions from model_loader.py and use them to load your models."
      ],
      "metadata": {
        "id": "zcaD67c7Dc36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask API is a micro web framework for Python, is like FastAPI  framework that loads your model and provides a prediction endpoint.\n",
        "# FastAPI is modern.\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from model_loader import text_classification_model, ner_model, sentiment_analysis_model\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post('/classify')\n",
        "async def classify_text(request_data: TextRequest):\n",
        "    text = request_data.text\n",
        "\n",
        "    # Call the text classification model using the loaded function\n",
        "    text_classification_result = text_classification_model(text)\n",
        "\n",
        "    # Call the NER model using the loaded function\n",
        "    ner_result = ner_model(text)\n",
        "\n",
        "    # Call the sentiment analysis model using the loaded function\n",
        "    sentiment_result = sentiment_analysis_model(text)\n",
        "\n",
        "    return {\n",
        "        \"text_classification_result\": text_classification_result,\n",
        "        \"ner_result\": ner_result,\n",
        "        \"sentiment_result\": sentiment_result,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WwJGnLQvDgMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the API\n",
        "\n",
        "To run your FastAPI application, open your terminal or command prompt and navigate to the directory where your main.py file is located.\n",
        "\n",
        "Then, run the following command:\n",
        "\n",
        "**uvicorn main:app --reload**"
      ],
      "metadata": {
        "id": "SFyEgwtO-Rp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Develop a Streamlit Web Application\n",
        "\n",
        "This Streamlit app allows users to input text and click the \"Analyze\" button. It then calls the API and displays the results for all three models: text classification, NER, and sentiment analysis.\n",
        "\n",
        "- Modify your Streamlit app to call the API and display the results for all three models."
      ],
      "metadata": {
        "id": "ZdhmQd2ayxni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "# Streamlit app\n",
        "st.title('NLP Model Demo')\n",
        "\n",
        "# Input text\n",
        "text = st.text_area('Enter text:', '')\n",
        "\n",
        "if st.button('Analyze'):\n",
        "    # Make a POST request to your API\n",
        "    response = requests.post('http://localhost:5000/classify', json={'text': text})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "        st.write('Text Classification Result:', results['text_classification_result'])\n",
        "        st.write('NER Result:', results['ner_result'])\n",
        "        st.write('Sentiment Analysis Result:', results['sentiment_result'])\n",
        "    else:\n",
        "        st.write('Error occurred during analysis.')\n"
      ],
      "metadata": {
        "id": "YG6qYQgWy49N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Run Streamlit App and see the results:\n",
        "\n",
        "- Make sure your FastAPI application (main.py) is already running using Uvicorn with the command:\n",
        "\n",
        "**uvicorn main:app --reload**\n",
        "\n",
        "Your FastAPI application should be running at http://localhost:8000.\n",
        "\n",
        "- Open a separate terminal or command prompt.\n",
        "\n",
        "- Navigate to the directory where your streamlit-app.py file is located.\n",
        "\n",
        "- Run your Streamlit app using the following command:\n",
        "\n",
        "**streamlit run streamlit-app.py**\n",
        "\n",
        "\n",
        "This command will start your Streamlit app and open it in a web browser window.\n",
        "\n",
        "In the Streamlit app, you can enter text in the text area and click the \"Analyze\" button. It will make a POST request to your FastAPI API, receive the results, and display them on the Streamlit app interface."
      ],
      "metadata": {
        "id": "kxqMSyosEpLI"
      }
    }
  ]
}